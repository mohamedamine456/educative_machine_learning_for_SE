{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataProcessingWithscikit-learn.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWCHyPoK2Bje"
      },
      "source": [
        "# **INTRODUCTION**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J5rdVRZ2GhV"
      },
      "source": [
        "# ML engineering vs. data science\n",
        "\n",
        "In industry, there is quite a bit of overlap between machine learning engineering and data science. Both jobs involve working with data, such as data analysis and data preprocessing.\n",
        "\n",
        "The main task for machine learning engineers is to first analyze the data for viable trends, then create an efficient input pipeline for training a model. This process involves using libraries like NumPy and pandas for handling data, along with machine learning frameworks like TensorFlow for creating the model and input pipeline.\n",
        "\n",
        "While the NumPy and pandas libraries are also used in data science, the Data Preprocessing section will cover one of the core libraries that is specific to industry-level data science: scikit-learn. Data scientists tend to work on smaller datasets than machine learning engineers, and their main goal is to analyze the data and quickly extract usable results. Therefore, they focus more on traditional data inference models (found in scikit-learn), rather than deep neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m16gGaKZ3W09"
      },
      "source": [
        "# **STANDARDIZING DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNIvpixR3bwk"
      },
      "source": [
        "# A- Standard Data format"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaB8Oj6R4yqq"
      },
      "source": [
        "Data can contain all sorts of different values. For example, Olympic 100m sprint times will range from 9.5 to 10.5 seconds, while calorie counts in large pepperoni pizzas can range from 1500 to 3000 calories. Even data measuring the exact same quantities can range in value (e.g. weight in kilograms vs. weight in pounds).\n",
        "\n",
        "When data can take on any range of values, it makes it difficult to interpret. Therefore, data scientists will convert the data into a standard format to make it easier to understand. The standard format refers to data that has 0 mean and unit variance (i.e. standard deviation = 1), and the process of converting data into this format is called data standardization.\n",
        "\n",
        "Data standardization is a relatively simple process. For each data value, x, we subtract the overall mean of the data, μ, then divide by the overall standard deviation, σ. The new value, z, represents the standardized data value. Thus, the formula for data standardization is:\n",
        "\n",
        "z = (x - μ) / σ\n",
        "​"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzvD6ya76E4x"
      },
      "source": [
        "# imports\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSYM-jk141uL",
        "outputId": "5b68fac7-5a22-4166-e997-b6ae9e2b288f"
      },
      "source": [
        "# B- NumPy and scikit-learn\n",
        "# define pizza_data\n",
        "pizza_data = np.array([[2100,   10,  800],\n",
        "                       [2500,   11,  850],\n",
        "                       [1800,   10,  760],\n",
        "                       [2000,   12,  800],\n",
        "                       [2300,   11,  810]])\n",
        "\n",
        "print('{}\\n'.format(repr(pizza_data)))\n",
        "print('{}\\n'.format(pizza_data.sum()))\n",
        "\n",
        "# import scale from sklearn.preprocessing\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "# standardizing each column of pizza_data\n",
        "col_standardized = scale(pizza_data)\n",
        "print('{}\\n'.format(repr(col_standardized)))\n",
        "\n",
        "# column means (rounded to nearest tousandth)\n",
        "col_means = col_standardized.mean(axis=0).round(decimals=3)\n",
        "print('{}\\n'.format(repr(col_means)))\n",
        "\n",
        "# column standard deviations\n",
        "col_stds = col_standardized.std(axis=0)\n",
        "print('{}\\n'.format(repr(col_stds)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "array([[2100,   10,  800],\n",
            "       [2500,   11,  850],\n",
            "       [1800,   10,  760],\n",
            "       [2000,   12,  800],\n",
            "       [2300,   11,  810]])\n",
            "\n",
            "14774\n",
            "\n",
            "array([[-0.16552118, -1.06904497, -0.1393466 ],\n",
            "       [ 1.4896906 ,  0.26726124,  1.60248593],\n",
            "       [-1.40693001, -1.06904497, -1.53281263],\n",
            "       [-0.57932412,  1.60356745, -0.1393466 ],\n",
            "       [ 0.66208471,  0.26726124,  0.2090199 ]])\n",
            "\n",
            "array([ 0., -0.,  0.])\n",
            "\n",
            "array([1., 1., 1.])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4U36se4D-fx"
      },
      "source": [
        "# **DATA RANGE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7a_myauEAvG"
      },
      "source": [
        "# A- Range scaling\n",
        "\n",
        "# To scale a data by compressing it into a fixed range. \n",
        "# one of the biggest use cases for this is compressig data into the range [0, 1].\n",
        "# helps to view data in terms of porportions, or percentages based on maximum and minimum valuesin data.\n",
        "# the formula for scaling based on a range is. a two-step process. for a given data value x,\n",
        "# we first compute the porportion of the value with respect to the min and max of the data\n",
        "# like:  Xprop = (X - Dmin) / (Dmax - Dmin)\n",
        "# we compute the proportion of the data value Xprop !!!!(works only if Dmin != Dmax)\n",
        "# then we use proprotion to scale to the range [Rmin, Rmax].\n",
        "# the formula to do that is: Xscale = Xprop * (Rmax - Rmin) + Rmin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b5wYJvtGhEX",
        "outputId": "ec24af6e-829d-47d4-f7c9-24cbbe6b091a"
      },
      "source": [
        "# B- Range compression in scikit-learn\n",
        "\n",
        "# define data\n",
        "data = np.array([[ 1.2,  3.2],\n",
        "                 [-0.3, -1.2],\n",
        "                 [ 6.5, 10.1],\n",
        "                 [ 2.2, -8.4]])\n",
        "\n",
        "print('{}\\n'.format(repr(data)))\n",
        "\n",
        "# import MinMaxScaler from sklearn.preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# default scaler range is [0, 1]\n",
        "default_scaler = MinMaxScaler()\n",
        "transformed = default_scaler.fit_transform(data)\n",
        "print('{}\\n'.format(repr(transformed)))\n",
        "\n",
        "# custom Scaler, example range [-2, 3]\n",
        "custom_scaler = MinMaxScaler(feature_range=(-2, 3))\n",
        "transformed = custom_scaler.fit_transform(data)\n",
        "print('{}\\n'.format(repr(transformed)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "array([[ 1.2,  3.2],\n",
            "       [-0.3, -1.2],\n",
            "       [ 6.5, 10.1],\n",
            "       [ 2.2, -8.4]])\n",
            "\n",
            "array([[0.22058824, 0.62702703],\n",
            "       [0.        , 0.38918919],\n",
            "       [1.        , 1.        ],\n",
            "       [0.36764706, 0.        ]])\n",
            "\n",
            "array([[-0.89705882,  1.13513514],\n",
            "       [-2.        , -0.05405405],\n",
            "       [ 3.        ,  3.        ],\n",
            "       [-0.16176471, -2.        ]])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUH8H-TDIGIs",
        "outputId": "1c0116f4-0769-4c5d-ea97-7e34bd8c4d12"
      },
      "source": [
        "# define new_data\n",
        "new_data = np.array([[ 1.2, -0.5],\n",
        "                     [ 5.3,  2.3],\n",
        "                     [-3.3,  4.1]])\n",
        "\n",
        "print('{}\\n'.format(repr(new_data)))\n",
        "\n",
        "# import MinMaxScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "default_scaler = MinMaxScaler()\n",
        "transformed = default_scaler.fit_transform(new_data)\n",
        "print('{}\\n'.format(repr(transformed)))\n",
        "\n",
        "# new instance of MinMaxScaler\n",
        "default_scaler = MinMaxScaler()\n",
        "# different data value fit\n",
        "default_scaler.fit(data)\n",
        "transformed = default_scaler.transform(new_data)\n",
        "print('{}\\n'.format(repr(transformed)))\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "array([[ 1.2, -0.5],\n",
            "       [ 5.3,  2.3],\n",
            "       [-3.3,  4.1]])\n",
            "\n",
            "array([[0.52325581, 0.        ],\n",
            "       [1.        , 0.60869565],\n",
            "       [0.        , 1.        ]])\n",
            "\n",
            "array([[ 0.22058824,  0.42702703],\n",
            "       [ 0.82352941,  0.57837838],\n",
            "       [-0.44117647,  0.67567568]])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQKvcufiJzpt"
      },
      "source": [
        "# **ROBUST SCALING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-fa6VglJ2Ii"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAj-SbOaJ2Xz"
      },
      "source": [
        "# A- Data Outliers\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}